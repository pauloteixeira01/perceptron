<h1 align="center">NEURAL NETWORK - PERCEPTRON</h1>
<p align="center">Made with: python with anaconda</p>

<p align="center">
  <img src="assets/python.jpeg" width="100px"  />
  <img src="assets/anaconda-python.png" width="100px" border-radius="12px"  />
</p>

<p align="center">
  <img src="assets/img-2.png" width="100%"/>

  <img src="assets/img-1.png" width="100%" />
  
  <img src="assets/img-3.png" width="100%" />
</p>

<h2 align="center">Transfer (Activation) Functions</h2>

<h3 align='center'>Unit step (threshold)</h3>
<p>
  The output is set at one of two levels, depending on whether the total input is greater than or less than some threshold value.
</p>
<p align="center">
  <img src="assets/unit-step.png" width="100%"/>
</p>

<h3 align='center'>Sigmoid</h3>
<p>
  The sigmoid function consists of 2 functions, logistic and tangential. The values of logistic function range from 0 and 1 and -1 to +1 for tangential function.
</p>
<p align="center">
  <img src="assets/sigmoid.png" width="100%"/>
</p>

<h3 align='center'>Hyperbolic tangent - (TanH)</h3>
<p>
  TanH (hyperbolic tangent) activation function and its derivative are defined by Eqs. (9.2) and (9.3), respectively TanH is a nonlinear activation function, with its center at 0 and its value ranging between â€“1 to 1 as shown in the graph in Fig. 9.4. It is mainly used in hidden layers, because its mean is 0 or near to it which helps in centering the data.[7] This eases the learning for the next layer.
</p>
<p align="center">
  <img src="assets/hyperbolic-tangent.png" width="100%"/>
</p>
